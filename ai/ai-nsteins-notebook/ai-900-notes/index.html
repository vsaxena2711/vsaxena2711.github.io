<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>📘 Notes from My AI-900 Journey: Azure AI Fundamentals | AInfraMinds</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q9KS4FM50S"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Q9KS4FM50S")</script><link rel=stylesheet href=/css/custom.css><title>📘 Notes from My AI-900 Journey: Azure AI Fundamentals | AInfraMinds</title><meta name=description content="Azure, Terraform, AI, DevOps tips and tutorials."><meta name=keywords content="Azure,Terraform,IaC,DevOps,AI,Terragrunt,Azure CLI,Cloud Engineering"><meta name=author content="Vaibhav Saxena"><meta name=robots content="index, follow"><meta property="og:title" content="📘 Notes from My AI-900 Journey: Azure AI Fundamentals"><meta property="og:description" content><meta property="og:url" content="https://ainframinds.com/ai/ai-nsteins-notebook/ai-900-notes/"><meta property="og:type" content="article"><meta name=twitter:card content="summary"><meta name=twitter:title content="📘 Notes from My AI-900 Journey: Azure AI Fundamentals"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"📘 Notes from My AI-900 Journey: Azure AI Fundamentals","description":"","author":{"@type":"Person","name":"Vaibhav Saxena"},"datePublished":"2025-03-01 00:00:00 \u002b0000 UTC"}</script></head><body><h1>AInfraMinds</h1><nav><ul><li><a href=/about/>About</a></li><li><a href=/contact/>Contact</a></li></ul></nav><main class=main><h2 id=-i-passed-the-ai-900-exam>🚀 I Passed the AI-900 Exam!</h2><p>I’m excited to share that I cleared the <strong>AI-900: Microsoft Azure AI Fundamentals</strong> exam! 🎉<br>While preparing, I created a set of personal notes — and I’m sharing them here in the hope that they’ll help others on the same path.</p><p>These notes aren’t a complete study guide, but they <em>are</em> great for quick reference, revision, and understanding the core concepts you’ll need to know for the exam.</p><hr><h2 id=-quick-navigation>🧭 Quick Navigation</h2><ul><li><a href=#-fundamental-ai-concepts>🧠 Fundamental AI Concepts</a></li><li><a href=#-fundamentals-of-machine-learning>🤖 Fundamentals of Machine Learning</a></li><li><a href=#-fundamentals-of-computer-vision>👁️ Fundamentals of Computer Vision</a></li><li><a href=#-fundamentals-of-facial-recognition>👤 Fundamentals of Facial Recognition</a></li><li><a href=#-fundamentals-of-optical-character-recognition>🔡 Fundamentals of Optical Character Recognition</a></li><li><a href=#-fundamentals-of-text-analysis-with-the-language-service>📚 Fundamentals of Text Analysis with the Language Service</a></li><li><a href=#-fundamentals-of-conversational-language-understanding>🗣️ Fundamentals of Conversational Language Understanding</a></li><li><a href=#-fundamentals-of-azure-ai-speech>🔊 Fundamentals of Azure AI Speech</a></li><li><a href=#-important-topics>📌 Important Topics</a></li></ul><hr><h2 id=-fundamental-ai-concepts>🧠 Fundamental AI Concepts</h2><h3 id=-key-workloads-of-ai>📊 Key Workloads of AI</h3><ul><li><strong>Machine Learning</strong> The foundation of many AI systems. It enables computers to learn from data and make predictions or decisions without being explicitly programmed.</li><li><strong>Computer Vision</strong> Enables machines to interpret and process visual information from images or videos.</li><li><strong>Natural Language Processing (NLP)</strong> Allows machines to understand, interpret, and generate human language — both spoken and written.</li><li><strong>Document Intelligence</strong> Helps process and extract data from structured and unstructured forms and documents.</li><li><strong>Knowledge Mining</strong> Extracts useful insights from large volumes of unstructured information and creates a searchable knowledge base.</li><li><strong>Generative AI</strong> Focuses on creating new content — such as text, code, or images — using models trained on large datasets.</li></ul><h3 id=-machine-learning-in-microsoft-azure>🧪 Machine Learning in Microsoft Azure</h3><p>Microsoft Azure provides the <strong>Azure Machine Learning service</strong> — a cloud-based platform for building, training, and deploying machine learning models.</p><h4 id=authoring-experiences>Authoring Experiences</h4><ul><li><strong>Automated Machine Learning</strong> Empowers non-experts to build effective machine learning models quickly from their data.</li><li><strong>Azure Machine Learning Designer</strong> A no-code, drag-and-drop interface to design machine learning workflows.</li><li><strong>Data Metric Visualization</strong> Helps analyze and optimize experiments using built-in visual tools.</li><li><strong>Notebooks</strong> Integrated Jupyter Notebooks for writing and running code directly in Azure Machine Learning Studio.</li></ul><h3 id=-computer-vision-in-microsoft-azure>👁️ Computer Vision in Microsoft Azure</h3><p>Computer vision enables AI to interpret the world visually — through images, video, or camera input.</p><h4 id=common-computer-vision-models>Common Computer Vision Models</h4><ul><li><strong>Image Classification</strong> – Classify an image into a predefined category.</li><li><strong>Object Detection</strong> – Detect and locate objects in images.</li><li><strong>Semantic Segmentation</strong> – Identify pixel-level boundaries of objects.</li><li><strong>Image Analysis</strong> – Extract tags, descriptions, and content.</li><li><strong>Face Detection, Analysis & Recognition</strong> – Recognize or verify individuals.</li><li><strong>Optical Character Recognition (OCR)</strong> – Read printed or handwritten text.</li></ul><h4 id=azure-ai-vision-capabilities>Azure AI Vision Capabilities</h4><ul><li><strong>Image Analysis</strong> Analyze images and extract descriptions, tags, text, and objects.</li><li><strong>Face</strong> Detect facial features, emotions, and perform recognition or verification.</li><li><strong>OCR (Optical Character Recognition)</strong> Extract text from scanned documents, handwritten notes, and images.</li></ul><h3 id=-natural-language-processing-nlp>🗣️ Natural Language Processing (NLP)</h3><p>Natural Language Processing enables software to understand and interact using human language — both written and spoken.</p><h4 id=what-can-nlp-do>What Can NLP Do?</h4><ul><li>Analyze and interpret text in documents, emails, and messages.</li><li>Interpret spoken language and generate spoken responses.</li><li>Translate content between different languages.</li><li>Understand intent behind commands and respond appropriately.</li></ul><h4 id=azure-services-for-nlp>Azure Services for NLP</h4><ul><li><strong>Azure AI Language</strong> – Analyze sentiment, extract key phrases, detect language, and more.</li><li><strong>Azure AI Speech</strong> – Transcribe speech to text, synthesize text to speech, translate spoken content.</li></ul><h3 id=-document-intelligence>📄 Document Intelligence</h3><p>Document Intelligence refers to the ability to process and extract structured data from unstructured or semi-structured content like forms, invoices, receipts, and documents.</p><h4 id=azure-document-intelligence-capabilities>Azure Document Intelligence Capabilities</h4><ul><li><strong>Azure AI Document Intelligence</strong> (formerly Form Recognizer) enables scanning, processing, and extracting text, tables, and key-value pairs from documents like PDFs or images.</li></ul><h3 id=-knowledge-mining>🔍 Knowledge Mining</h3><p>Knowledge mining involves extracting meaningful insights from large volumes of unstructured data — such as PDFs, Office docs, and other content — and making that data searchable.</p><h4 id=azure-services-for-knowledge-mining>Azure Services for Knowledge Mining</h4><ul><li><strong>Azure AI Search</strong> – Combines indexing, natural language processing, and cognitive skills to make unstructured content searchable and insightful.</li></ul><h3 id=-generative-ai>✨ Generative AI</h3><p>Generative AI enables machines to create content — not just analyze it. This includes:</p><ul><li>Natural language (text)</li><li>Images</li><li>Code</li><li>Audio</li><li>Video</li></ul><h4 id=azure-generative-ai-capabilities>Azure Generative AI Capabilities</h4><ul><li><strong>Azure OpenAI Service</strong><br>Gives access to powerful language models (e.g., GPT-4, Codex, DALL·E) to build intelligent, generative solutions within your applications.</li></ul><h3 id=-responsible-ai>🔐 Responsible AI</h3><p>Building AI responsibly is critical. Microsoft outlines six key principles that guide the ethical development and deployment of AI.</p><h4 id=principles-of-responsible-ai>Principles of Responsible AI</h4><ul><li><p><strong>Fairness</strong><br>AI systems should treat everyone fairly and avoid bias (e.g., gender, race).</p></li><li><p><strong>Reliability and Safety</strong><br>AI must be tested to operate safely and consistently — especially in critical areas like healthcare or transportation.</p></li><li><p><strong>Privacy and Security</strong><br>AI should respect data privacy and protect information at every step of its lifecycle.</p></li><li><p><strong>Inclusiveness</strong><br>AI should be accessible and beneficial to all people, regardless of ability or background.</p></li><li><p><strong>Transparency</strong><br>AI systems should be understandable, and users should know how decisions are made.</p></li><li><p><strong>Accountability</strong><br>Humans must remain accountable for AI behavior and ensure ethical compliance.</p></li></ul><hr><h2 id=-fundamentals-of-machine-learning>🤖 Fundamentals of Machine Learning</h2><h3 id=-types-of-machine-learning>🧠 Types of Machine Learning</h3><p><img src=/images/ai-notebook/ml-types-diagram.png alt="Types of Machine Learning"></p><h3 id=-supervised-machine-learning>🎯 Supervised Machine Learning</h3><p>Supervised machine learning is a general term for machine learning algorithms in which the training data includes both <strong>feature values</strong> and <strong>known label values</strong>.</p><p>These algorithms learn relationships between features and labels from past data, allowing them to predict unknown labels for future data.</p><h4 id=-regression>📉 Regression</h4><p>Regression is a type of supervised learning where the label is a <strong>numeric value</strong>.</p><p>Examples include:</p><ul><li>The number of ice creams sold on a given day, based on temperature, rainfall, and windspeed.</li><li>The selling price of a property based on square footage, number of bedrooms, and neighborhood stats.</li><li>The fuel efficiency (miles-per-gallon) of a car based on engine size, weight, and dimensions.</li></ul><h4 id=-classification>🧾 Classification</h4><p>Classification is another type of supervised learning, where the label is a <strong>category</strong> (class).</p><h5 id=-binary-classification>✅ Binary Classification</h5><p>The model predicts one of <strong>two</strong> mutually exclusive outcomes.</p><p>Examples:</p><ul><li>Whether a patient is at risk for diabetes based on metrics like weight, glucose level, etc.</li><li>Whether a customer will default on a loan based on income, credit score, etc.</li><li>Whether a marketing campaign will receive a positive response based on demographics and purchase history.</li></ul><h5 id=-multiclass-classification>🧩 Multiclass Classification</h5><p>The model predicts one <strong>class out of multiple possibilities</strong>.</p><p>Examples:</p><ul><li>Predicting the <strong>species</strong> of a penguin (Adelie, Gentoo, or Chinstrap) from physical traits.</li><li>Classifying the <strong>genre</strong> of a movie (comedy, horror, romance, etc.) based on its cast, director, and budget.</li></ul><h3 id=-unsupervised-machine-learning>🧬 Unsupervised Machine Learning</h3><p>Unsupervised machine learning uses data that has <strong>no labels</strong> — only feature values.</p><p>These algorithms learn to identify hidden patterns or structures in the data.</p><h4 id=-clustering>🔗 Clustering</h4><p>The most common form of unsupervised learning is clustering.</p><p>It groups observations based on similarities in their features.</p><p>Examples:</p><ul><li>Grouping similar flowers based on size, leaf count, and petal count.</li><li>Identifying customer segments based on demographics and shopping behavior.</li></ul><hr><h2 id=-fundamentals-of-computer-vision>👁️ Fundamentals of Computer Vision</h2><h3 id=-machine-learning-for-computer-vision>🤖 Machine Learning for Computer Vision</h3><h4 id=-convolutional-neural-networks-cnns>🧠 Convolutional Neural Networks (CNNs)</h4><p>One of the most common machine learning model architectures for computer vision is a <strong>convolutional neural network (CNN)</strong>.<br>CNNs use filters to extract numeric <strong>feature maps</strong> from images, which are then processed by deep learning layers to generate predictions such as classification labels.</p><h4 id=-transformers-and-multi-modal-models>🔀 Transformers and Multi-modal Models</h4><p>CNNs have traditionally been the foundation of many vision-based models. They&rsquo;re commonly used for <strong>image classification</strong>, and serve as the backbone for more complex tasks like <strong>object detection</strong>, which combines CNN-based feature extraction with region identification to detect and classify multiple objects within an image.</p><h5 id=-transformers>🧩 Transformers</h5><p>While CNNs have driven most advances in computer vision, another architecture—<strong>transformers</strong>—has transformed natural language processing (NLP). Transformers have enabled large, capable language models, and their adaptability has inspired researchers to explore them for vision tasks too.</p><h5 id=-multi-modal-models>🌐 Multi-modal Models</h5><p>Inspired by transformers in NLP, researchers have developed <strong>multi-modal models</strong> that learn from large volumes of <strong>captioned images</strong> (images with textual descriptions), rather than traditional fixed labels. These models are trained to understand both visual and linguistic data, improving their ability to generalize across modalities.</p><h3 id=-azure-ai-vision>🖼️ Azure AI Vision</h3><p>You can use either of the following Azure resource types to implement vision AI capabilities:</p><ul><li><strong>Azure AI Vision</strong>: A dedicated resource for computer vision tasks. Choose this if you want to isolate billing, utilization, and access specifically for vision.</li><li><strong>Azure AI Services</strong>: A broader resource that includes vision, language, translation, and more. Choose this for multi-service projects where integration and cost management across services is needed.</li></ul><h3 id=-analyzing-images-with-azure-ai-vision>🧪 Analyzing Images with Azure AI Vision</h3><p>Azure AI Vision supports several out-of-the-box analysis features, including:</p><ul><li><strong>Optical Character Recognition (OCR)</strong> – Extracts printed or handwritten text from images.</li><li><strong>Image Captioning</strong> – Generates natural language descriptions of image content.</li><li><strong>Object Detection</strong> – Identifies and locates thousands of common objects.</li><li><strong>Visual Tagging</strong> – Assigns descriptive tags to visual elements within the image.</li></ul><h3 id=-training-custom-models>🧰 Training Custom Models</h3><h4 id=-image-classification>🏷️ Image Classification</h4><p>Use custom image classification to predict <strong>what</strong> category an image belongs to.<br>For example, a trained model could identify whether an image contains an apple, banana, or orange.</p><h4 id=-object-detection>📦 Object Detection</h4><p>Object detection models go further by identifying <strong>where</strong> each object is in the image and what it is.<br>These models return bounding box coordinates for each object detected.<br>You can train object detection models using your own dataset—such as images of fruits—to detect multiple objects in a single image.</p><hr><h2 id=-fundamentals-of-facial-recognition>👤 Fundamentals of Facial Recognition</h2><h3 id=-uses-of-face-detection-and-analysis>🔍 Uses of Face Detection and Analysis</h3><p>There are many applications for face detection, analysis, and recognition. For example:</p><ul><li><strong>Security</strong> – Facial recognition can be used in building access control systems and smartphone unlocking mechanisms.</li><li><strong>Social Media</strong> – Automatically tag friends in photos using facial recognition.</li><li><strong>Intelligent Monitoring</strong> – In automobiles, detect if the driver is distracted, drowsy, or not looking at the road.</li><li><strong>Advertising</strong> – Analyze faces to tailor ads to the appropriate demographic audience.</li><li><strong>Missing Persons</strong> – Use public surveillance systems to identify missing individuals.</li><li><strong>Identity Validation</strong> – Helpful at border entry kiosks using special access permits.</li></ul><h3 id=-understand-face-analysis>🧠 Understand Face Analysis</h3><p>Face detection involves identifying regions in an image that contain a human face by returning <strong>bounding box coordinates</strong>.</p><p>With <strong>face analysis</strong>, facial landmarks such as the nose, eyes, eyebrows, and lips are identified and used to derive insights or feed downstream machine learning models.</p><h3 id=-facial-recognition>🧬 Facial Recognition</h3><p>Facial recognition takes facial analysis a step further. It involves training a machine learning model with <strong>multiple images</strong> of a known individual so that the model can later <strong>identify them in unseen images</strong>. This enables accurate identity matching across image datasets.</p><h3 id=-get-started-with-face-analysis-on-azure>⚙️ Get Started with Face Analysis on Azure</h3><p>Microsoft Azure offers various services for face detection and analysis:</p><ul><li><strong>Azure AI Vision</strong> – Detects faces and returns basic face attributes such as bounding boxes.</li><li><strong>Azure AI Video Indexer</strong> – Identifies faces in video content using advanced vision capabilities.</li><li><strong>Azure AI Face</strong> – Offers pre-trained models for detecting, analyzing, and recognizing faces in still images.</li></ul><h3 id=-face-service-attributes>🧾 Face Service Attributes</h3><p>The Azure Face service can return the coordinates of human faces and provide additional attributes, including:</p><ul><li><strong>Accessories</strong> – Identifies glasses, masks, headwear, etc., along with a confidence score.</li><li><strong>Blur</strong> – Measures how blurry the face appears.</li><li><strong>Exposure</strong> – Analyzes underexposure or overexposure on the detected face.</li><li><strong>Glasses</strong> – Detects if the person is wearing glasses.</li><li><strong>Head Pose</strong> – Captures 3D orientation of the head.</li><li><strong>Mask</strong> – Indicates whether the person is wearing a mask.</li><li><strong>Noise</strong> – Measures image graininess which may affect face clarity.</li><li><strong>Occlusion</strong> – Detects if the face is partially covered or blocked by other objects.</li></ul><h3 id=-azure-resources-for-face>🔧 Azure Resources for Face</h3><p>To use the Face service in Azure, you can create one of the following resource types:</p><ul><li><strong>Face</strong> – A dedicated resource for Azure AI Face, best if you want isolated billing and usage tracking.</li><li><strong>Azure AI Services</strong> – A consolidated resource that supports Face along with other services like Language, Content Safety, and more.</li></ul><hr><h2 id=-fundamentals-of-optical-character-recognition>🔡 Fundamentals of Optical Character Recognition</h2><h3 id=-azure-ai-visions-ocr-engine>🧠 Azure AI Vision&rsquo;s OCR Engine</h3><p>The <strong>Azure AI Vision</strong> service enables the extraction of machine-readable text from images using its powerful <strong>Read API</strong>. This OCR engine supports images, PDFs, and TIFF files, and is optimized for general, non-document images — making it ideal for user-facing applications that involve text recognition.</p><p>When you call the <strong>Read API</strong>, it returns results in a structured hierarchy:</p><ul><li><strong>Pages</strong> – Each page of text, along with page size and orientation.</li><li><strong>Lines</strong> – Lines of text found on each page.</li><li><strong>Words</strong> – Words within the lines, including the bounding box and text content.</li></ul><h3 id=-get-started-with-vision-studio-on-azure>🚀 Get Started with Vision Studio on Azure</h3><p>To begin using <strong>Azure AI Vision</strong>, you’ll need to create a resource in your Azure subscription. You have two options:</p><ul><li><strong>Azure AI Vision</strong> – A dedicated resource for vision services. Best suited if you&rsquo;re only using Vision and want separate tracking of costs and usage.</li><li><strong>Azure AI Services</strong> – A broader resource that includes Vision, Language, Speech, and other services. Ideal for managing multiple AI services under a single umbrella.</li></ul><hr><h2 id=-fundamentals-of-text-analysis-with-the-language-service>📚 Fundamentals of Text Analysis with the Language Service</h2><h3 id=-introduction-to-nlp-and-azure-ai-language>💬 Introduction to NLP and Azure AI Language</h3><p>Natural Language Processing (NLP) enables computer systems to interpret written or spoken language in a way that’s similar to how humans do. <strong>Text analysis</strong> is a branch of NLP that focuses on extracting meaningful insights from unstructured text.</p><p><strong>Azure AI Language</strong> is a cloud-based service that provides features for text understanding — such as sentiment analysis, key phrase extraction, summarization, and more.</p><h3 id=-understand-text-analytics>🔍 Understand Text Analytics</h3><h4 id=-tokenization>🧩 Tokenization</h4><p>The first step in analyzing text is breaking it into smaller components called <strong>tokens</strong>. Tokens can be individual words, punctuation marks, or combinations.</p><p>Example phrase: <code>"we choose to go to the moon"</code></p><p>Tokenized:</p><ol><li>we</li><li>choose</li><li>to</li><li>go</li><li>the</li><li>moon</li></ol><p>Token sequence: <code>[1, 2, 3, 4, 3, 5, 6]</code></p><h4 id=-frequency-analysis>📈 Frequency Analysis</h4><p>After tokenizing, you can count how often each token appears. Common terms (excluding stop words like “a” or “the”) often reveal the document’s subject.<br>For example:</p><ul><li>Frequent words: <code>new</code>, <code>go</code>, <code>space</code>, <code>moon</code></li><li>Common bi-gram: <code>the moon</code><br>The context implies that the subject is space travel.</li></ul><h4 id=-machine-learning-for-text-classification>🤖 Machine Learning for Text Classification</h4><p>You can train machine learning models to classify text using algorithms like <strong>logistic regression</strong>.<br>Use case: <strong>Sentiment analysis</strong></p><p>Labeled examples:</p><ul><li>&ldquo;The food and service were both great&rdquo; → 1 (positive)</li><li>&ldquo;A really terrible experience&rdquo; → 0 (negative)</li><li>&ldquo;Mmm! tasty food and a fun vibe&rdquo; → 1 (positive)</li><li>&ldquo;Slow service and substandard food&rdquo; → 0 (negative)</li></ul><h4 id=-semantic-language-models>🧠 Semantic Language Models</h4><p>Modern NLP leverages <strong>semantic embeddings</strong>, which encode tokens as numeric vectors. These vector-based representations help models understand meaning and relationships between words.</p><h3 id=-get-started-with-text-analysis-in-azure>🚀 Get Started with Text Analysis in Azure</h3><p><strong>Azure AI Language</strong> supports a variety of NLP capabilities:</p><ul><li><strong>Named Entity Recognition (NER)</strong> – Identifies people, places, events, and more. Customizable categories are also supported.</li><li><strong>Entity Linking</strong> – Matches known entities and links them (e.g., to Wikipedia).</li><li><strong>PII Detection</strong> – Identifies sensitive personal or health-related info.</li><li><strong>Language Detection</strong> – Returns language name (<code>English</code>), code (<code>en</code>), and confidence score.</li><li><strong>Sentiment Analysis</strong> – Labels sentences as positive, neutral, or negative.</li><li><strong>Summarization</strong> – Condenses long text to highlight core insights.</li><li><strong>Key Phrase Extraction</strong> – Extracts essential points from large text bodies.</li></ul><h3 id=-entity-recognition-and-linking>🧾 Entity Recognition and Linking</h3><p>Submit unstructured text and get back identified <strong>entities</strong> — categorized items like people, locations, or dates.</p><h3 id=-language-detection>🌍 Language Detection</h3><p>Azure detects:</p><ul><li>Language name (e.g., English)</li><li>ISO 639-1 code (e.g., <code>en</code>)</li><li>Confidence score</li></ul><p>Useful for multilingual applications and localization tasks.</p><h3 id=-sentiment-analysis-and-opinion-mining>❤️ Sentiment Analysis and Opinion Mining</h3><p>Azure’s sentiment analysis returns:</p><ul><li>Sentiment label (positive, neutral, or negative)</li><li>Confidence scores</li><li>Sentence-level granularity</li></ul><p>Used in analyzing reviews, social media posts, support tickets, etc.</p><h3 id=-key-phrase-extraction>✨ Key Phrase Extraction</h3><p>Summarizes the main points of a document by listing essential phrases.<br>For example, from many restaurant reviews, Azure AI Language can help extract:</p><ul><li><code>Great service</code></li><li><code>Delicious food</code></li><li><code>Long wait time</code></li></ul><hr><h2 id=-fundamentals-of-conversational-language-understanding>🗣️ Fundamentals of Conversational Language Understanding</h2><h3 id=-describe-conversational-language-understanding>🧠 Describe Conversational Language Understanding</h3><p>Conversational language understanding enables applications to interpret user input in natural language conversations. It focuses on three main concepts: <strong>utterances</strong>, <strong>entities</strong>, and <strong>intents</strong>.</p><h4 id=-utterances>💬 Utterances</h4><p>An <strong>utterance</strong> is something a user might say.</p><p>Examples:</p><ul><li>&ldquo;Switch the fan on.&rdquo;</li><li>&ldquo;Turn on the light.&rdquo;</li></ul><p>These represent inputs the system should understand and interpret meaningfully.</p><h4 id=-entities>🧩 Entities</h4><p>An <strong>entity</strong> is a specific item or concept mentioned in an utterance.</p><p>In the examples above:</p><ul><li><code>"fan"</code> and <code>"light"</code> are entities — both are types of <strong>device</strong>.</li></ul><p>Entities help the system identify what the user is talking about.</p><h4 id=-intents>🎯 Intents</h4><p>An <strong>intent</strong> represents the goal or purpose of an utterance.</p><p>Both examples aim to <strong>turn a device on</strong>, so the intent might be labeled as <code>TurnOn</code>.</p><p>Your conversational model groups related utterances under defined intents and identifies entities within them.</p><blockquote><p>🧠 Think of it like this:<br>&ldquo;Turn on the fan.&rdquo; → Intent = <code>TurnOn</code>, Entity = <code>fan</code></p></blockquote><h3 id=-building-a-conversational-model-in-azure>🛠️ Building a Conversational Model in Azure</h3><p>To build a conversational AI model in Azure, you define:</p><ul><li><strong>Entities</strong> (e.g., device names)</li><li><strong>Intents</strong> (e.g., TurnOn, TurnOff)</li><li><strong>Utterances</strong> to train the model</li></ul><p>Azure uses these to train your model to understand user inputs.</p><p>You might build models for various scenarios such as:</p><ul><li>Turning devices on/off</li><li>Setting reminders</li><li>Booking appointments</li></ul><h3 id=-azure-resources-for-conversational-ai>🧰 Azure Resources for Conversational AI</h3><p>To use conversational understanding in Azure, you need a resource:</p><ul><li><p><strong>Azure AI Language</strong><br>Best for authoring and predicting. Offers a complete development and deployment experience.</p></li><li><p><strong>Azure AI Services</strong><br>A general-purpose resource. Can be used for <strong>prediction only</strong>.</p></li></ul><h3 id=-authoring-the-model>✍️ Authoring the Model</h3><p>Once you have a resource, you define:</p><ul><li><strong>Intents</strong> (like <code>TurnOn</code>, <code>TurnOff</code>)</li><li><strong>Entities</strong> (like <code>fan</code>, <code>light</code>)</li><li><strong>Utterances</strong> (example user phrases)</li></ul><p>Azure also offers <strong>prebuilt domains</strong> — collections of pre-defined intents and entities to help you get started.</p><p>You can use these or create custom ones tailored to your application.</p><h3 id=-training-the-model>🏋️ Training the Model</h3><p>Training uses your sample utterances to &ldquo;teach&rdquo; the model how to match user input to intents and entities.</p><p>For example:</p><ul><li>Input: <code>"Switch on the heater"</code></li><li>Model Prediction: <code>Intent = TurnOn</code>, <code>Entity = heater</code></li></ul><p>Training helps the model generalize so it can understand variations in language.</p><h3 id=-predicting-with-the-model>🚀 Predicting with the Model</h3><p>After testing and validating your model, you publish it to a <strong>prediction resource</strong>.</p><p>Client applications (chatbots, mobile apps, websites) then:</p><ul><li>Send user input to the prediction endpoint</li><li>Receive a response containing:<ul><li>The <strong>predicted intent</strong></li><li>The <strong>identified entities</strong></li></ul></li></ul><p>Based on the prediction, the app can then perform an appropriate action.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;intent&#34;</span>: <span style=color:#e6db74>&#34;TurnOn&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;entities&#34;</span>: [<span style=color:#e6db74>&#34;light&#34;</span>]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><hr><h2 id=-fundamentals-of-azure-ai-speech>🔊 Fundamentals of Azure AI Speech</h2><p>AI speech capabilities allow us to interact with technology through voice — whether that’s managing home systems, asking questions aloud, generating captions, or receiving spoken responses.</p><p>To enable this interaction, AI systems must support two core capabilities:</p><ul><li><strong>Speech Recognition</strong> – Understanding spoken language and converting it into text.</li><li><strong>Speech Synthesis</strong> – Generating spoken output from text.</li></ul><p>Azure provides both through the <strong>Azure AI Speech</strong> service, supporting prebuilt and custom models for use cases like real-time transcription, custom voice creation, and more.</p><h3 id=-understand-speech-recognition-and-synthesis>🧠 Understand Speech Recognition and Synthesis</h3><h4 id=-speech-recognition>🗣️ Speech Recognition</h4><p>Speech recognition converts spoken input (live or recorded) into machine-readable data, typically in the form of text.</p><p>It involves:</p><ul><li><strong>Acoustic Models</strong> – Convert the audio signal into <strong>phonemes</strong> (basic sound units).</li><li><strong>Language Models</strong> – Map phonemes to words using statistical algorithms.</li></ul><p>Common use cases:</p><ul><li>Live captions and subtitles for videos</li><li>Automated meeting transcripts</li><li>Dictation tools</li><li>Voice command interfaces</li></ul><h4 id=-speech-synthesis>🗣️ Speech Synthesis</h4><p>Speech synthesis, or <strong>Text-to-Speech (TTS)</strong>, generates spoken output from text.</p><p>To synthesize speech, the system:</p><ul><li>Tokenizes text into words</li><li>Assigns phonetic values to each word</li><li>Breaks phonemes into phrases or sentences</li><li>Generates audio with desired <strong>voice</strong>, <strong>pitch</strong>, <strong>volume</strong>, and <strong>speed</strong></li></ul><p>Use cases include:</p><ul><li>Voice assistants and bots</li><li>Telephone IVRs</li><li>Email readers for accessibility</li><li>Public announcement systems</li></ul><h3 id=-get-started-with-speech-on-azure>🚀 Get Started with Speech on Azure</h3><p>Azure’s <strong>AI Speech service</strong> provides both recognition and synthesis via the following APIs:</p><ul><li><strong>Speech to Text API</strong> – Convert spoken language to written text.</li><li><strong>Text to Speech API</strong> – Convert written text to spoken voice output.</li></ul><h4 id=-azure-resource-options>🧰 Azure Resource Options</h4><p>To use Azure AI Speech, create one of the following resources:</p><ul><li><strong>Speech Resource</strong> – For projects focused solely on speech capabilities.</li><li><strong>Azure AI Services Resource</strong> – For multi-service solutions using Speech, Language, Vision, etc.</li></ul><h3 id=-speech-to-text-api>🎤 Speech to Text API</h3><h4 id=-real-time-transcription>🔴 Real-time Transcription</h4><p>Transcribe audio as it happens — ideal for live presentations, demos, or accessibility scenarios.</p><h4 id=-batch-transcription>📦 Batch Transcription</h4><p>Upload audio files (e.g., from file shares or Azure Storage via SAS URI) and receive transcription results asynchronously.</p><h3 id=-text-to-speech-api>🗣️ Text to Speech API</h3><p>With the <strong>Text to Speech API</strong>, you can specify custom <strong>voices</strong>, making your application sound unique and engaging.</p><p>Applications include:</p><ul><li>Responding to user queries</li><li>Interactive phone systems</li><li>Accessibility tools (e.g., email reading)</li><li>Smart environments with vocal output</li></ul><hr><h2 id=-important-topics>📌 Important Topics</h2><h3 id=-binary-classification-evaluation-metrics>🧮 Binary Classification Evaluation Metrics</h3><p>The first step in evaluating a binary classification model is creating a <strong>confusion matrix</strong>, comparing predicted labels (ŷ) with actual labels (y):</p><table><thead><tr><th>Actual / Predicted</th><th>0 (Negative)</th><th>1 (Positive)</th></tr></thead><tbody><tr><td>0 (Negative)</td><td>TN</td><td>FP</td></tr><tr><td>1 (Positive)</td><td>FN</td><td>TP</td></tr></tbody></table><p>Where:</p><ul><li><strong>TN</strong> – True Negative</li><li><strong>FP</strong> – False Positive</li><li><strong>FN</strong> – False Negative</li><li><strong>TP</strong> – True Positive</li></ul><h4 id=-accuracy>✅ Accuracy</h4><p>Proportion of total predictions that were correct:<br><strong>(TP + TN) / (TP + TN + FP + FN)</strong></p><h4 id=-recall-sensitivity>📢 Recall (Sensitivity)</h4><p>Proportion of actual positives correctly identified:<br><strong>TP / (TP + FN)</strong></p><h4 id=-precision>🎯 Precision</h4><p>Proportion of predicted positives that are actually positive:<br><strong>TP / (TP + FP)</strong></p><h4 id=-f1-score>⚖️ F1-Score</h4><p>Harmonic mean of Precision and Recall:<br><strong>(2 × Precision × Recall) / (Precision + Recall)</strong></p><h4 id=-area-under-the-curve-auc>📈 Area Under the Curve (AUC)</h4><p>AUC measures performance by plotting <strong>True Positive Rate (TPR)</strong> against <strong>False Positive Rate (FPR)</strong>:</p><ul><li>TPR = TP / (TP + FN)</li><li>FPR = FP / (FP + TN)
AUC is <strong>threshold-invariant</strong>, meaning it evaluates model quality regardless of threshold used.</li></ul><hr><h3 id=-metrics-for-classification-models>📋 Metrics for Classification Models</h3><ul><li><strong>Accuracy</strong> – Proportion of all correct predictions.</li><li><strong>Precision</strong> – TP / (TP + FP)</li><li><strong>Recall</strong> – TP / (TP + FN)</li><li><strong>F1 Score</strong> – Weighted average of precision and recall.</li><li><strong>AUC</strong> – Area under the ROC curve.</li></ul><hr><h3 id=-metrics-for-regression-models>📊 Metrics for Regression Models</h3><p>Used to estimate the <strong>amount of error</strong> between predicted and actual values.</p><ul><li><strong>Mean Absolute Error (MAE)</strong> – Average of absolute differences.</li><li><strong>Root Mean Squared Error (RMSE)</strong> – Square root of average squared errors.</li><li><strong>Relative Absolute Error (RAE)</strong> – Ratio of total absolute error to the total absolute error of the mean.</li><li><strong>Relative Squared Error (RSE)</strong> – Ratio of squared error to the total squared error of the mean.</li><li><strong>R² (Coefficient of Determination)</strong> – Predictive power; 0 = no predictive value, 1 = perfect fit.</li></ul><hr><h3 id=-metrics-for-clustering-models>📌 Metrics for Clustering Models</h3><p>Used to measure the compactness and separation of clusters:</p><ul><li><strong>Average Distance to Other Center</strong> – How far each point in a cluster is from other cluster centroids.</li><li><strong>Average Distance to Cluster Center</strong> – Closeness of points to their own cluster center.</li><li><strong>Maximal Distance to Cluster Center</strong> – Furthest distance within the cluster.</li><li><strong>Number of Points</strong> – Count of points assigned to each cluster.</li><li><strong>Combined Evaluation Score</strong> – Overall average performance across all clusters.</li></ul><hr><h3 id=-text-analytics-capabilities>🧠 Text Analytics Capabilities</h3><ul><li><strong>Sentiment Analysis</strong> – Detects sentiment as <em>Positive</em>, <em>Neutral</em>, or <em>Negative</em>.</li><li><strong>Key Phrase Extraction</strong> – Pulls main ideas from unstructured text.</li><li><strong>Entity Recognition</strong> – Identifies people, places, organizations, etc.</li><li><strong>Language Detection</strong> – Identifies the language of given text.</li></ul><hr><h3 id=-entity-types-in-luis-language-understanding>🧱 Entity Types in LUIS (Language Understanding)</h3><p>While authoring a LUIS application, you can define entities as:</p><ul><li><strong>Machine-Learned</strong> – AI learns how to extract them from examples.</li><li><strong>List</strong> – Predefined list of possible values.</li><li><strong>RegEx</strong> – Match patterns with regular expressions.</li><li><strong>Pattern.any</strong> – Wildcard-based matching.</li></ul><hr><h3 id=-four-steps-of-data-transformation>🔁 Four Steps of Data Transformation</h3><ul><li><strong>Feature Selection</strong> – Selecting relevant variables.</li><li><strong>Removing Outliers</strong> – Cleaning data for better generalization.</li><li><strong>Impute Missing Values</strong> – Filling in missing data points.</li><li><strong>Normalize Numeric Values</strong> – Scaling values to a standard range.</li></ul><hr><h3 id=-sentiment-analysis-categories>💬 Sentiment Analysis Categories</h3><ul><li><strong>Positive</strong> – Expresses joy, satisfaction, excitement, etc.</li><li><strong>Neutral</strong> – Objective or factual; no strong emotion.</li><li><strong>Negative</strong> – Expresses disappointment, frustration, or dissatisfaction.</li></ul><div style=text-align:center;margin-top:2rem><a href=/ai/ai-nsteins-notebook/ class=home-button>👉 Return to AI-nstein's Notebook</a><br><br><a href=/ai/ class=home-button>📦 Return to AI World</a><br><br><a href=/ class=home-button>🏠 Take Me Home</a></div><script src=https://giscus.app/client.js data-repo=vsaxena2711/vsaxena2711.github.io data-repo-id=REPO_ID data-category=General data-category-id=CATEGORY_ID data-mapping=pathname data-reactions-enabled=1 data-theme=light crossorigin=anonymous async></script></main><p>Copyright 2025. All rights reserved.</p></body></html>